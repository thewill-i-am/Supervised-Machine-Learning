{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\npytz: No module named 'pytz'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-84987bc6d994>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/pandas/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mmissing_dependencies\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m     raise ImportError(\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0;34m\"Unable to import required dependencies:\\n\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmissing_dependencies\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     )\n",
      "\u001B[0;31mImportError\u001B[0m: Unable to import required dependencies:\npytz: No module named 'pytz'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pandas import DataFrame\n",
    "from matplotlib import colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "os.chdir(\"/home/william/Desktop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datosTicTacToe = pd.read_csv(\"tic-tac-toe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for x in datosTicTacToe:\n",
    "    if x != \" Gana-x\":\n",
    "        datosTicTacToe[x] = datosTicTacToe[x].astype('category')\n",
    "datosTicTacToe.dtypes\n",
    "\n",
    "datosTicTacToe = datosTicTacToe.replace({'x': 0, 'o': 1, 'b': 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PrediccionBase:\n",
    "    def __init__(self, datos):\n",
    "        self.__datos = self.cleaning(datos)\n",
    "        self.__precisionGlobal = 0\n",
    "        self.__error_global = 0\n",
    "        self.__verdaderosNegativos = 0\n",
    "        self.__falsosPositivos = 0\n",
    "        self.__falsosNegativos = 0\n",
    "        self.__verdaderosPositivos = 0\n",
    "        self.__reporte = 0\n",
    "        self.__precision_category = 0\n",
    "\n",
    "    @property\n",
    "    def datos(self):\n",
    "        return self.__datos\n",
    "\n",
    "    @property\n",
    "    def reporte(self):\n",
    "        return self.__reporte\n",
    "\n",
    "    def cleaning(self, datos):\n",
    "        datos = datos.replace({'x': 0, 'o': 1, 'b': 2})\n",
    "        return datos\n",
    "\n",
    "    def entrenamiento(self):\n",
    "        pass\n",
    "\n",
    "    def generacionReporte(self, nombreDelModelo):\n",
    "        dict = {\n",
    "            \"Modelo\": [nombreDelModelo],\n",
    "            \"Precision Global\": [self.__precisionGlobal],\n",
    "            \"Error Global\": [self.__error_global],\n",
    "            \"Verdaderos Positivos\": [self.__verdaderosPositivos],\n",
    "            \"Verdaderos Negativos\": [self.__verdaderosNegativos],\n",
    "            \"Falsos Negativos\": [self.__falsosNegativos],\n",
    "            \"Falsos Positivos\": [self.__falsosPositivos]}\n",
    "        self.__reporte = pd.DataFrame(dict).join(self.__precision_category)\n",
    "\n",
    "    def analsis(self, MC, modelo):\n",
    "        self.__verdaderosNegativos, self.__falsosPositivos, self.__falsosNegativos, self.__verdaderosPositivos = MC.ravel()\n",
    "        self.__precisionGlobal = np.sum(MC.diagonal()) / np.sum(MC)\n",
    "        self.__error_global = 1 - self.__precisionGlobal\n",
    "        self.__precision_category = pd.DataFrame(MC.diagonal() / np.sum(MC, axis=1)).T\n",
    "        self.__precision_category.columns = [\"Precision Positiva (PP)\", \"Precision Negativa (PN)\"]\n",
    "        self.generacionReporte(modelo)\n",
    "        return {\"Matriz de Confusión\": MC,\n",
    "                \"Precisión Global\": self.__precisionGlobal,\n",
    "                \"Error Global\": self.__error_global,\n",
    "                \"Precisión por categoría\": self.__precision_category}\n",
    "\n",
    "\n",
    "class PrediccionKNeighbors(PrediccionBase):\n",
    "    def __init__(self, datos):\n",
    "        super().__init__(datos)\n",
    "        self.__instancia_potenciacion = None\n",
    "        self.__x_test = None\n",
    "\n",
    "    @property\n",
    "    def instancia(self):\n",
    "        return self.__instancia_potenciacion\n",
    "\n",
    "    @property\n",
    "    def x_test(self):\n",
    "        return self.__x_test\n",
    "\n",
    "    def entrenamiento(self, nucleo=\"auto\", n_neighbors=3, train_size=0.80, entrenamiento=\"KNeighbors\"):\n",
    "        x = self.datos.iloc[:, :-1]\n",
    "        y = self.datos.iloc[:, -1]\n",
    "        X_train, self.__x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=0)\n",
    "        self.__instancia_potenciacion = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=nucleo)\n",
    "        self.__instancia_potenciacion.fit(X_train, y_train)\n",
    "        prediccion = self.__instancia_potenciacion.predict(self.__x_test)\n",
    "        MC = confusion_matrix(y_test, prediccion)\n",
    "        indices = self.analsis(MC, entrenamiento)\n",
    "        # for k in indices:\n",
    "        #     print(\"\\n%s:\\n%s\" % (k, str(indices[k])))\n",
    "\n",
    "\n",
    "class PrediccionADABoosting(PrediccionBase):\n",
    "    def __init__(self, datos):\n",
    "        super().__init__(datos)\n",
    "        self.__instancia = None\n",
    "        self.__instancia_potenciacion = None\n",
    "        self.__x_train = []\n",
    "\n",
    "    @property\n",
    "    def instancia(self):\n",
    "        return self.__instancia_potenciacion\n",
    "\n",
    "    @property\n",
    "    def x_test(self):\n",
    "        return self.__x_test\n",
    "\n",
    "    def obtenerVariablesImportantes(self):\n",
    "        importancia = self.__instancia_potenciacion.feature_importances_\n",
    "        print(importancia)\n",
    "        etiquetas = self.__x_train.columns.values\n",
    "        y_pos = np.arange(len(etiquetas))\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(y_pos, importancia, align='center', alpha=0.5)\n",
    "        plt.yticks(y_pos, etiquetas)\n",
    "\n",
    "    def entrenamiento(self, train_size=0.75, criterion=\"gini\", splitter=\"best\", min_samples_split=2,\n",
    "                      entretenamiento=\"Bosques Aleatorios ADA Boosting\"):\n",
    "        x = self.datos.iloc[:, :-1]\n",
    "        y = self.datos.iloc[:, -1]\n",
    "        self.__instancia = DecisionTreeClassifier(min_samples_split=min_samples_split, max_depth=None,\n",
    "                                                  criterion=criterion, splitter=splitter)\n",
    "        self.__instancia_potenciacion = AdaBoostClassifier(base_estimator=self.__instancia,\n",
    "                                                           n_estimators=100, random_state=0)\n",
    "        self.__x_train, self.__x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=0)\n",
    "        self.__instancia_potenciacion.fit(self.__x_train, y_train)\n",
    "        prediccion = self.__instancia_potenciacion.predict(self.__x_test)\n",
    "        MC = confusion_matrix(y_test, prediccion)\n",
    "        indices = self.analsis(MC, entretenamiento)\n",
    "        # for k in indices:\n",
    "        #     print(\"\\n%s:\\n%s\" % (k, str(indices[k])))\n",
    "\n",
    "\n",
    "class PrediccionXGBoosting(PrediccionBase):\n",
    "    def __init__(self, datos):\n",
    "        super().__init__(datos)\n",
    "        self.__instancia = None\n",
    "        self.__instancia_potenciacion = None\n",
    "        self.__x_train = []\n",
    "\n",
    "    @property\n",
    "    def x_test(self):\n",
    "        return self.__x_test\n",
    "    @property\n",
    "    def instancia(self):\n",
    "        return self.__instancia_potenciacion\n",
    "\n",
    "    def obtenerVariablesImportantes(self):\n",
    "        importancia = self.__instancia_potenciacion.feature_importances_\n",
    "        print(importancia)\n",
    "        etiquetas = self.__x_train.columns.values\n",
    "        y_pos = np.arange(len(etiquetas))\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(y_pos, importancia, align='center', alpha=0.5)\n",
    "        plt.yticks(y_pos, etiquetas)\n",
    "\n",
    "    def entrenamiento(self, train_size=0.75, n_estimators=10, random_state=0, min_samples_split=2,\n",
    "                      entrenamiento=\"Bosques Aleatorios XG Boosting\"):\n",
    "        x = self.datos.iloc[:, :-1]\n",
    "        y = self.datos.iloc[:, -1]\n",
    "\n",
    "        self.__instancia_potenciacion = GradientBoostingClassifier(n_estimators=n_estimators, random_state=n_estimators,\n",
    "                                                                   min_samples_split=min_samples_split, max_depth=None)\n",
    "\n",
    "        self.__x_train, self.__x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=0)\n",
    "        self.__instancia_potenciacion.fit(self.__x_train, y_train)\n",
    "        prediccion = self.__instancia_potenciacion.predict(self.__x_test)\n",
    "        MC = confusion_matrix(y_test, prediccion)\n",
    "        indices = self.analsis(MC, entrenamiento)\n",
    "        # for k in indices:\n",
    "        #     print(\"\\n%s:\\n%s\" % (k, str(indices[k])))\n",
    "\n",
    "\n",
    "class PrediccionRandomForest(PrediccionBase):\n",
    "    def __init__(self, datos):\n",
    "        super().__init__(datos)\n",
    "        self.__instancia = None\n",
    "        self.__instancia_potenciacion = None\n",
    "        self.__x_train = []\n",
    "\n",
    "    @property\n",
    "    def instancia(self):\n",
    "        return self.__instancia_potenciacion\n",
    "\n",
    "    @property\n",
    "    def x_test(self):\n",
    "        return self.__x_test\n",
    "\n",
    "    def obtenerVariablesImportantes(self):\n",
    "        importancia = self.__instancia_potenciacion.feature_importances_\n",
    "        print(importancia)\n",
    "        etiquetas = self.__x_train.columns.values\n",
    "        y_pos = np.arange(len(etiquetas))\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(y_pos, importancia, align='center', alpha=0.5)\n",
    "        plt.yticks(y_pos, etiquetas)\n",
    "\n",
    "    def entrenamiento(self, train_size=0.75, n_estimators=10, entrenamiento=\"Bosques Aleatorios\"):\n",
    "        x = self.datos.iloc[:, :-1]\n",
    "        y = self.datos.iloc[:, -1]\n",
    "\n",
    "        self.__instancia_potenciacion = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "\n",
    "        self.__x_train, self.__x_test, y_train, y_test = train_test_split(x, y, train_size=train_size, random_state=0)\n",
    "        self.__instancia_potenciacion.fit(self.__x_train, y_train)\n",
    "        prediccion = self.__instancia_potenciacion.predict(self.__x_test)\n",
    "        MC = confusion_matrix(y_test, prediccion)\n",
    "        indices = self.analsis(MC, entrenamiento)\n",
    "        # for k in indices:\n",
    "        #     print(\"\\n%s:\\n%s\" % (k, str(indices[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "class Analisis_Predictivo:\n",
    "\n",
    "    def __init__(self,datos:DataFrame, predecir:str, predictoras = [],\n",
    "                 modelo = None,train_size = 80,random_state = None):\n",
    "        '''\n",
    "        datos: Datos completos y listos para construir un modelo\n",
    "\n",
    "        modelo: Instancia de una Clase de un método de clasificación(KNN,Árboles,SVM,etc).\n",
    "        Si no especifica un modelo no podrá utilizar el método fit_n_review()\n",
    "\n",
    "        predecir: Nombre de la variable a predecir\n",
    "\n",
    "        predictoras: Lista de los nombres de las variables predictoras.\n",
    "        Si vacío entonces utiliza todas las variables presentes excepto la variable a predecir.\n",
    "\n",
    "        train_size: Proporción de la tabla de entrenamiento respecto a la original.\n",
    "\n",
    "        random_state: Semilla aleatoria para la división de datos(training-testing).\n",
    "        '''\n",
    "        self.datos = datos\n",
    "        self.predecir = predecir\n",
    "        self.predictoras = predictoras\n",
    "        self.modelo = modelo\n",
    "        self.random_state = random_state\n",
    "        if modelo != None:\n",
    "            self.train_size = train_size\n",
    "            self._training_testing()\n",
    "\n",
    "\n",
    "    def _training_testing(self):\n",
    "        if len(self.predictoras) == 0:\n",
    "            X = self.datos.drop(columns=[self.predecir])\n",
    "        else:\n",
    "            X = self.datos[self.predictoras]\n",
    "\n",
    "        y = self.datos[self.predecir].values\n",
    "\n",
    "        train_test = train_test_split(X, y, train_size=self.train_size,\n",
    "                                      random_state=self.random_state)\n",
    "        self.X_train, self.X_test,self.y_train, self.y_test = train_test\n",
    "\n",
    "\n",
    "    def fit_predict(self):\n",
    "        if self.modelo != None:\n",
    "            self.modelo.fit(self.X_train,self.y_train)\n",
    "            return self.modelo.predict(self.X_test)\n",
    "\n",
    "    def fit_predict_resultados(self, imprimir = True):\n",
    "        if(self.modelo != None):\n",
    "            y = self.datos[self.predecir].values\n",
    "            prediccion = self.fit_predict()\n",
    "            MC = confusion_matrix(self.y_test, prediccion)\n",
    "            indices = self.indices_general(MC,list(np.unique(y)))\n",
    "            if imprimir == True:\n",
    "                for k in indices:\n",
    "                    print(\"\\n%s:\\n%s\"%(k,str(indices[k])))\n",
    "\n",
    "            return indices\n",
    "\n",
    "    def indices_general(self,MC, nombres = None):\n",
    "        \"Método para calcular los índices de calidad de la predicción\"\n",
    "        precision_global = np.sum(MC.diagonal()) / np.sum(MC)\n",
    "        error_global = 1 - precision_global\n",
    "        precision_categoria  = pd.DataFrame(MC.diagonal()/np.sum(MC,axis = 1)).T\n",
    "        if nombres!=None:\n",
    "            precision_categoria.columns = nombres\n",
    "        return {\"Matriz de Confusión\":MC,\n",
    "                \"Precisión Global\":precision_global,\n",
    "                \"Error Global\":error_global,\n",
    "                \"Precisión por categoría\":precision_categoria}\n",
    "\n",
    "    def distribucion_variable_predecir(self):\n",
    "        \"Método para graficar la distribución de la variable a predecir\"\n",
    "        variable_predict = self.predecir\n",
    "        data = self.datos\n",
    "        colors = list(dict(**mcolors.CSS4_COLORS))\n",
    "        df = pd.crosstab(index=data[variable_predict],columns=\"valor\") / data[variable_predict].count()\n",
    "        fig = plt.figure(figsize=(10,9))\n",
    "        g = fig.add_subplot(111)\n",
    "        countv = 0\n",
    "        titulo = \"Distribución de la variable %s\" % variable_predict\n",
    "        for i in range(df.shape[0]):\n",
    "            g.barh(1,df.iloc[i],left = countv, align='center',color=colors[11+i],label= df.iloc[i].name)\n",
    "            countv = countv + df.iloc[i]\n",
    "        vals = g.get_xticks()\n",
    "        g.set_xlim(0,1)\n",
    "        g.set_yticklabels(\"\")\n",
    "        g.set_title(titulo)\n",
    "        g.set_ylabel(variable_predict)\n",
    "        g.set_xticklabels(['{:.0%}'.format(x) for x in vals])\n",
    "        countv = 0\n",
    "        for v in df.iloc[:,0]:\n",
    "            g.text(np.mean([countv,countv+v]) - 0.03, 1 , '{:.1%}'.format(v), color='black', fontweight='bold')\n",
    "            countv = countv + v\n",
    "        g.legend(loc='upper center', bbox_to_anchor=(1.08, 1), shadow=True, ncol=1)\n",
    "\n",
    "    def poder_predictivo_categorica(self, var:str):\n",
    "        \"Método para ver la distribución de una variable categórica respecto a la predecir\"\n",
    "        data = self.datos\n",
    "        variable_predict = self.predecir\n",
    "        df = pd.crosstab(index= data[var],columns=data[variable_predict])\n",
    "        df = df.div(df.sum(axis=1),axis=0)\n",
    "        titulo = \"Distribución de la variable %s según la variable %s\" % (var,variable_predict)\n",
    "        g = df.plot(kind='barh',stacked=True,legend = True, figsize = (10,9), \\\n",
    "                    xlim = (0,1),title = titulo, width = 0.8)\n",
    "        vals = g.get_xticks()\n",
    "        g.set_xticklabels(['{:.0%}'.format(x) for x in vals])\n",
    "        g.legend(loc='upper center', bbox_to_anchor=(1.08, 1), shadow=True, ncol=1)\n",
    "        for bars in g.containers:\n",
    "            plt.setp(bars, width=.9)\n",
    "        for i in range(df.shape[0]):\n",
    "            countv = 0\n",
    "            for v in df.iloc[i]:\n",
    "                g.text(np.mean([countv,countv+v]) - 0.03, i , '{:.1%}'.format(v), color='black', fontweight='bold')\n",
    "                countv = countv + v\n",
    "\n",
    "\n",
    "    def poder_predictivo_numerica(self,var:str):\n",
    "        \"Función para ver la distribución de una variable numérica respecto a la predecir\"\n",
    "        sns.FacetGrid(self.datos, hue=self.predecir, height=6).map(sns.kdeplot, var, shade=True).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver en el siguiente grafico nos estamos enfrentando a un problema un poco desequilibrado, por que los resultados positivos son mucho mas frecuentes que los negativos, hablando en cifras un 30.6% mas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "analisisPredictivo = Analisis_Predictivo(datosTicTacToe, predecir=\" Gana-x\")\n",
    "analisisPredictivo.distribucion_variable_predecir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podemos ver el poder predictivo de las variables del dataset de tic tac toe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for x in datosTicTacToe:\n",
    "    if x != \" Gana-x\":\n",
    "        analisisPredictivo.poder_predictivo_numerica(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bayes = GaussianNB()\n",
    "analisisPredictivoBayes = Analisis_Predictivo(datosTicTacToe,predecir= \" Gana-x\",modelo=bayes,\n",
    "                                       train_size= 0.8, random_state=0)\n",
    "print(\"Las predicciones en Testing son: {}\".format(analisisPredictivoBayes.fit_predict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resultados = analisisPredictivoBayes.fit_predict_resultados()\n",
    "precisionGlobal = pd.DataFrame({\"Precisión Global\" : [resultados['Precisión Global']]})\n",
    "errorGlobal = pd.DataFrame({\"Error Global\" : [resultados['Error Global']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precisonCategoria = resultados['Precisión por categoría']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui abajo podemos ver que la precision global es de un 69% lo cual no esta mal, pero tenemos modelos que predicen mejor este problema.\n",
    "\n",
    "El valor de la precision positiva tiene trampa por que basicamente predice casi todo positivo, por eso tiene un valor tan alto, y eso lo podemos confirmar cuando vemos el valor negativo que muy bajo 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "finalDataFrame = precisionGlobal.join(errorGlobal).join(precisonCategoria)\n",
    "finalDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediccionADABosting = PrediccionADABoosting(datos=datosTicTacToe)\n",
    "prediccionADABosting.entrenamiento(train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediccionXGBoosting = PrediccionXGBoosting(datos=datosTicTacToe)\n",
    "prediccionXGBoosting.entrenamiento(train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediccionRandomForest = PrediccionRandomForest(datos=datosTicTacToe)\n",
    "prediccionRandomForest.entrenamiento(train_size=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Aqui podemos ver que el resultado de los modelos como Decision Trees son mejoran mucho mas la prediccion\n",
    "\n",
    "Teniendo una precision global de mas del 88% en contraste del 69% del metodo de bayes el mejor modelo es el de  Bosques Aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reporteFinal = pd.concat([prediccionADABosting.reporte, prediccionXGBoosting.reporte, prediccionRandomForest.reporte])\n",
    "reporteFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datosWine = pd.read_csv(\"wine.csv\")\n",
    "datosWine[\"tipo\"] = datosWine[\"tipo\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bayes = GaussianNB()\n",
    "analisisPredictivoBayesWine = Analisis_Predictivo(datosWine,predecir= \"tipo\",modelo=bayes,\n",
    "                                       train_size= 0.75, random_state=0)\n",
    "\n",
    "print(\"Las predicciones en Testing son: {}\".format(analisisPredictivoBayesWine.fit_predict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "En la proxima celda voy a mostrar el analisis del poder predictivo de cada variables para saber cuales son las 6 mas influyentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for x in datosWine:\n",
    "    if x != \"tipo\":\n",
    "        analisisPredictivoBayesWine.poder_predictivo_numerica(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resultados = analisisPredictivoBayesWine.fit_predict_resultados()\n",
    "precisionGlobal = pd.DataFrame({\"Precisión Global\" : [resultados['Precisión Global']]})\n",
    "errorGlobal = pd.DataFrame({\"Error Global\" : [resultados['Error Global']]})\n",
    "finalDataFrame = precisionGlobal.join(errorGlobal).join(precisonCategoria)\n",
    "finalDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediccionADABosting = PrediccionADABoosting(datos=datosWine)\n",
    "prediccionADABosting.entrenamiento(train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediccionXGBoosting = PrediccionXGBoosting(datos=datosWine)\n",
    "prediccionXGBoosting.entrenamiento(train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediccionRandomForest = PrediccionRandomForest(datos=datosWine)\n",
    "prediccionRandomForest.entrenamiento(train_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podemos ver la precision con otros metodos como los bosques aleatorios, al parecer tiene unos resultados bastantes similares, pero realmente hay que notar que los metodos basados en arboles de decision superan en gran medida al metodo de prediccion de bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reporteFinal = pd.concat([prediccionADABosting.reporte, prediccionXGBoosting.reporte, prediccionRandomForest.reporte])\n",
    "reporteFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Asi queda la prediccion con la variables mas importantes, realmente no hay mucha diferencia, pero parece que predice un poco peor pero muy poco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bayes = GaussianNB()\n",
    "analisisPredictivoBayesWineFiltrado = Analisis_Predictivo(datosWine,predecir= \"tipo\",predictoras= [\"volatil.acidez\", \"cloruros\", \"total.sulfuro.dioxido\",\n",
    "                                             \"libre.sulfuro.dioxido\", \"sulfitos\", \"densidad\"], modelo=bayes,\n",
    "                                       train_size= 0.75, random_state=0)\n",
    "\n",
    "print(\"Las predicciones en Testing son: {}\".format(analisisPredictivoBayesWineFiltrado.fit_predict()))\n",
    "resultados = analisisPredictivoBayesWineFiltrado.fit_predict_resultados()\n",
    "precisionGlobal = pd.DataFrame({\"Precisión Global\" : [resultados['Precisión Global']]})\n",
    "errorGlobal = pd.DataFrame({\"Error Global\" : [resultados['Error Global']]})\n",
    "finalDataFrame = precisionGlobal.join(errorGlobal).join(precisonCategoria)\n",
    "finalDataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}